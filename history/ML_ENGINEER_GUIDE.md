# ğŸ¤– SIEM Anomaly Detector - ML Engineer Guide

## ğŸ“‹ Ãndice

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Arquitectura ML](#arquitectura-ml)
3. [Pipeline Completo](#pipeline-completo)
4. [Modelos del Ensemble](#modelos-del-ensemble)
5. [Feature Engineering](#feature-engineering)
6. [Training & Validation](#training--validation)
7. [MÃ©tricas de Performance](#mÃ©tricas-de-performance)
8. [Deployment & Production](#deployment--production)
9. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ IntroducciÃ³n

### Â¿QuÃ© es este sistema?

Sistema de detecciÃ³n de anomalÃ­as en logs de seguridad (SIEM) usando **Machine Learning no supervisado**. Detecta amenazas en tiempo real analizando patrones en logs de SSH, Nginx, syslog y firewall.

### Â¿Por quÃ© ML No Supervisado?

```
âœ… No requiere logs etiquetados (difÃ­cil de conseguir)
âœ… Detecta amenazas nunca vistas antes (0-day attacks)
âœ… Se adapta a patrones cambiantes
âŒ Puede tener falsos positivos (mitigado con ensemble)
```

### Stack TecnolÃ³gico

```python
# ML/Data Science
- scikit-learn 1.8.0    # ML algorithms
- numpy 2.4.1           # Numerical computing
- pandas 2.3.3          # Data manipulation
- joblib 1.5.3          # Model persistence

# Backend
- FastAPI 0.128.0       # API framework
- PostgreSQL 15         # Data storage
- Redis 7               # Rate tracking
- Pydantic 2.12.5       # Data validation
- structlog 25.5.0      # Structured logging

# Infrastructure
- Docker + Compose      # Containerization
- Prometheus + Grafana  # Monitoring
- uvicorn               # ASGI server
```

---

## ğŸ—ï¸ Arquitectura ML

### Diagrama de Alto Nivel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAW SECURITY LOG                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PARSERS (backend/parsers/)                                 â”‚
â”‚  - auth.py     â†’ SSH, sudo, su                              â”‚
â”‚  - nginx.py    â†’ HTTP access logs                           â”‚
â”‚  - syslog.py   â†’ Generic syslog                             â”‚
â”‚  - firewall.py â†’ iptables, ufw                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                  Structured Dict
                  {timestamp, ip, user, ...}
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FEATURE ENGINEERING (backend/ml/features.py)               â”‚
â”‚  - Query Redis    â†’ Rates (login attempts, req/sec)         â”‚
â”‚  - Query Postgres â†’ Historical (last activity, IPs)         â”‚
â”‚  - GeoIP lookup   â†’ Geographic distance, country            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  Output: 21 numerical features [0, 1, 0, ...]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML ENSEMBLE (backend/ml/ensemble.py)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Isolation    â”‚ DBSCAN       â”‚ GMM          â”‚            â”‚
â”‚  â”‚ Forest       â”‚              â”‚              â”‚            â”‚
â”‚  â”‚ (50%)        â”‚ (30%)        â”‚ (20%)        â”‚            â”‚
â”‚  â”‚              â”‚              â”‚              â”‚            â”‚
â”‚  â”‚ Score: 0.85  â”‚ Score: 0.75  â”‚ Score: 0.92  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                            â†“                                â”‚
â”‚  Weighted Sum: 0.5Ã—0.85 + 0.3Ã—0.75 + 0.2Ã—0.92 = 0.834      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT                                                     â”‚
â”‚  - is_anomaly: true                                         â”‚
â”‚  - risk_score: 0.834 (HIGH)                                 â”‚
â”‚  - recommended_action: BLOCK_IP                             â”‚
â”‚  - reasons: ["Unusual hour", "High login rate", ...]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Pipeline Completo

### 1. Ingesta de Logs

```python
# VÃ­a API REST
POST /api/v1/logs/analyze
{
  "log_line": "Jan 14 03:45:12 server sshd: Failed password for admin from 185.234.219.45",
  "source": "auth"
}

# Futuro: VÃ­a syslog UDP (puerto 514)
# Futuro: VÃ­a file watcher (tail -f /var/log/auth.log)
```

### 2. Parsing

```python
# backend/parsers/auth.py
class AuthLogParser:
    def parse(self, log_line: str) -> ParsedLog:
        # Regex patterns para extraer:
        # - timestamp
        # - hostname
        # - process (sshd, sudo, etc.)
        # - source_ip
        # - username
        # - event_type (ssh_password_failed, etc.)
        
# Output:
{
    "timestamp": "2026-01-14T03:45:12Z",
    "source_ip": "185.234.219.45",
    "username": "admin",
    "event_type": "ssh_password_failed",
    "success": false
}
```

### 3. Feature Engineering

```python
# backend/ml/features.py
class FeatureEngineer:
    async def extract(self, parsed_log: dict) -> LogFeatures:
        """
        Extrae 21 features en tiempo real:
        
        1. Temporal (4): hour, day_of_week, is_weekend, is_business_hours
        2. Frequency (4): login_attempts/min, requests/sec, unique_ips, endpoints
        3. Rates (3): failed_auth_rate, error_4xx, error_5xx
        4. Geographic (3): distance_km, is_known_country, is_known_ip
        5. Behavioral (4): bytes_transferred, time_since_last, session_duration, entropy
        6. Context (3): is_privileged_user, is_sensitive_endpoint, is_known_user_agent
        """
        
        # Consulta Redis para rates (Ãºltimos 60s)
        login_attempts = await self.redis.get(f"login_attempts:{source_ip}:last_minute")
        
        # Consulta PostgreSQL para histÃ³rico
        last_activity = await self.db.query(
            "SELECT MAX(timestamp) FROM logs WHERE source_ip = $1",
            source_ip
        )
        
        # GeoIP lookup
        geo = self.geoip.lookup(source_ip)
        distance_km = self._calculate_distance(geo.location, typical_location)
        
        return LogFeatures(
            hour_of_day=3,
            login_attempts_per_minute=25.0,  # â† RED FLAG
            failed_auth_rate=0.95,            # â† RED FLAG
            geographic_distance_km=8500.0,    # â† RED FLAG
            # ... 18 mÃ¡s
        )
```

### 4. ML Prediction

```python
# backend/ml/ensemble.py
class AnomalyEnsemble:
    def predict(self, features: LogFeatures) -> AnomalyResult:
        X = features.to_array().reshape(1, -1)  # [21 features]
        X_scaled = self.scaler.transform(X)
        
        # 1. Isolation Forest (50%)
        if_decision = self.isolation_forest.decision_function(X)[0]
        if_score = 1.0 / (1.0 + np.exp(if_decision * 10))
        # if_score = 0.85 (outlier!)
        
        # 2. DBSCAN (30%)
        dbscan_prediction = self._predict_dbscan(X_scaled[0])
        dbscan_score = 0.75  # Lejos de clusters
        
        # 3. GMM (20%)
        gmm_log_likelihood = self.gmm.score_samples(X_scaled)[0]
        gmm_score = 1.0 / (1.0 + np.exp((gmm_log_likelihood + 10) * 0.5))
        # gmm_score = 0.92 (baja probabilidad)
        
        # Weighted sum
        final_score = 0.5*0.85 + 0.3*0.75 + 0.2*0.92 = 0.834
        
        # Threshold (configurable via settings)
        is_anomaly = final_score >= 0.6  # medium threshold
        
        return AnomalyResult(
            is_anomaly=True,
            risk_score=0.834,
            confidence="high",  # Basado en std de scores
            isolation_forest_score=0.85,
            dbscan_score=0.75,
            gmm_score=0.92,
            important_features=[...],
            processing_time_ms=8.5
        )
```

---

## ğŸ¤– Modelos del Ensemble

### 1. Isolation Forest (50% peso)

**Algoritmo:**
```
1. Crea Ã¡rboles de decisiÃ³n aleatorios (n_estimators=100)
2. En cada split, elige feature y valor aleatorio
3. AnomalÃ­as â†’ pocas divisiones para aislarlas
4. Datos normales â†’ muchas divisiones
```

**ParÃ¡metros:**
```python
IsolationForest(
    contamination=0.05,    # Espera 5% anomalÃ­as
    n_estimators=100,      # 100 Ã¡rboles
    max_samples="auto",    # Auto-tune sample size
    random_state=42,       # Reproducibilidad
    n_jobs=-1              # Usa todos los cores
)
```

**Por quÃ© lo usamos:**
- âœ… Muy rÃ¡pido (O(n log n))
- âœ… Escala bien a datasets grandes
- âœ… No asume distribuciÃ³n de datos
- âœ… Detecta outliers globales

**Mejor para:**
- Ataques externos (IPs desconocidas)
- Patrones nunca vistos (0-day)
- Brute force attacks
- Port scanning

### 2. DBSCAN (30% peso)

**Algoritmo:**
```
1. Agrupa puntos densos en clusters
2. ParÃ¡metros: eps (radio) y min_samples (mÃ­n vecinos)
3. Puntos lejos de clusters â†’ outliers
4. No necesita nÃºmero de clusters predefinido
```

**ParÃ¡metros:**
```python
DBSCAN(
    eps=5.0,            # Radio de vecindad
    min_samples=50,     # MÃ­nimo puntos para core point
    n_jobs=-1
)
```

**Por quÃ© lo usamos:**
- âœ… Detecta anomalÃ­as locales
- âœ… Forma clusters de forma arbitraria
- âœ… Robusto a ruido
- âŒ No tiene predict() nativo (implementamos workaround)

**Mejor para:**
- Insider threats (comportamiento anÃ³malo de usuarios conocidos)
- Privilege escalation
- AnomalÃ­as temporales (actividad a horas raras)

### 3. Gaussian Mixture Model (20% peso)

**Algoritmo:**
```
1. Modela datos como mezcla de K gaussianas
2. Estima parÃ¡metros con EM algorithm
3. Calcula log-likelihood de cada punto
4. Baja probabilidad â†’ anomalÃ­a
```

**ParÃ¡metros:**
```python
GaussianMixture(
    n_components=3,         # 3 distribuciones
    covariance_type="full", # Covarianza completa
    random_state=42,
    n_init=10               # 10 inicializaciones
)
```

**Por quÃ© lo usamos:**
- âœ… Da scores probabilÃ­sticos (interpretables)
- âœ… Soft clustering (pertenencia parcial)
- âœ… Modela distribuciones complejas
- âŒ Asume gaussianidad (mitigado con ensemble)

**Mejor para:**
- AnomalÃ­as estadÃ­sticas
- Eventos raros pero vÃ¡lidos
- Drift detection (cambios graduales)

---

## ğŸ”§ Feature Engineering

### Features Calculados (21 total)

#### 1. Temporal Features (4)

```python
hour_of_day: int        # 0-23 (3 AM = sospechoso)
day_of_week: int        # 0-6 (0=Lunes, fin de semana = sospechoso)
is_weekend: bool        # True si sÃ¡bado/domingo
is_business_hours: bool # True si 9 AM - 6 PM
```

**Fuente:** `timestamp` del log parseado

**Por quÃ© importantes:**
- Ataques suelen ocurrir de noche (3-5 AM)
- Actividad en fin de semana es inusual
- Actividad fuera de horario laboral es sospechosa

#### 2. Frequency Features (4)

```python
login_attempts_per_minute: float  # Rate de logins (brute force)
requests_per_second: float        # Rate de requests (DDoS)
unique_ips_last_hour: int         # IPs distintas (distributed attack)
unique_endpoints_accessed: int    # Endpoints tocados (scanning)
```

**Fuente:** Redis (ventana deslizante de 60s)

**ImplementaciÃ³n:**
```python
# Redis key pattern
f"login_attempts:{source_ip}:last_minute"

# Incrementa counter con TTL
await redis.incr(key, expire=60)

# Calcula rate
count = await redis.get(key)
rate = count / 60.0
```

**Por quÃ© importantes:**
- Brute force â†’ >20 intentos/min
- DDoS â†’ >100 requests/sec
- Scanning â†’ >30 endpoints

#### 3. Rate Features (3)

```python
failed_auth_rate: float  # 0.0-1.0 (% de fallos)
error_rate_4xx: float    # Client errors (bad requests)
error_rate_5xx: float    # Server errors (DoS symptoms)
```

**Fuente:** Redis (Ãºltimos 5 minutos)

**Por quÃ© importantes:**
- Failed auth >70% = brute force
- High 4xx = scanning/probing
- High 5xx = DoS o exploit

#### 4. Geographic Features (3)

```python
geographic_distance_km: float  # Distancia desde ubicaciÃ³n tÃ­pica
is_known_country: bool         # PaÃ­s en whitelist
is_known_ip: bool              # IP en whitelist
```

**Fuente:** MaxMind GeoIP2 database

**ImplementaciÃ³n:**
```python
# GeoIP lookup
location = geoip2.city(source_ip)

# Calcular distancia haversine
distance = haversine(
    (location.lat, location.lon),
    (typical_lat, typical_lon)
)

# Check whitelists
is_known = source_ip in known_ips_set
is_known_country = location.country in ["US", "ES", "FR", "DE", "GB"]
```

**Por quÃ© importantes:**
- Login desde China cuando usuario estÃ¡ en EspaÃ±a = sospechoso
- Distancia >5000km = muy sospechoso
- IPs conocidas = usuarios legÃ­timos

#### 5. Behavioral Features (4)

```python
bytes_transferred: float           # log1p(bytes) normalizado
time_since_last_activity_sec: float # Inactividad antes del log
session_duration_sec: float         # DuraciÃ³n de sesiÃ³n
payload_entropy: float              # EntropÃ­a del payload (0-8)
```

**Fuente:** PostgreSQL (histÃ³rico) + anÃ¡lisis de payload

**ImplementaciÃ³n:**
```python
# EntropÃ­a (randomness del payload)
from scipy.stats import entropy

def calculate_entropy(data: str) -> float:
    """Shannon entropy - detects encrypted/random data"""
    probabilities = [data.count(c) / len(data) for c in set(data)]
    return entropy(probabilities, base=2)

# entropy â‰ˆ 4.5 = texto normal
# entropy â‰ˆ 7.8 = datos encriptados (shellcode, cryptominers)
```

**Por quÃ© importantes:**
- High entropy = payloads encriptados o shellcode
- Time since last = dormant accounts activating
- Bytes transferred = data exfiltration

#### 6. Context Features (3)

```python
is_privileged_user: bool      # root, admin, administrator
is_sensitive_endpoint: bool   # /admin, /api/admin, /wp-admin
is_known_user_agent: bool     # User agent en whitelist
```

**Fuente:** ConfiguraciÃ³n + anÃ¡lisis de log

**Por quÃ© importantes:**
- Privileged user + failed login = crÃ­tico
- Sensitive endpoint + unknown IP = alerta
- Unknown user agent = bot/script

### Feature Normalization

```python
class FeatureEngineer:
    def normalize(self, features: dict) -> np.ndarray:
        """
        Normaliza features a [0, 1] para el modelo
        """
        # Temporal features â†’ ya normalizados
        hour_norm = features["hour_of_day"] / 24.0
        day_norm = features["day_of_week"] / 7.0
        
        # Frequency features â†’ log scale + clip
        login_norm = min(features["login_attempts_per_minute"] / 30.0, 1.0)
        
        # Geographic â†’ log scale
        distance_norm = min(np.log1p(features["geographic_distance_km"]) / 10.0, 1.0)
        
        # Boolean â†’ 0 o 1
        is_weekend_norm = float(features["is_weekend"])
        
        return np.array([...])  # 21 features normalizados
```

---

## ğŸ“ Training & Validation

### Training Script

```bash
# Script profesional con mÃ©tricas completas
python scripts/train_ensemble_with_metrics.py

# Output:
# - Train/Val/Test split (60/20/20)
# - Data leakage check
# - Baseline comparison (Dummy Classifier)
# - Comprehensive metrics (F1, Precision, Recall, ROC-AUC, Confusion Matrix)
# - Model saved to models/
# - Metrics saved to JSON
```

### Data Split Strategy

```python
# 1. First split: 80% train+val, 20% test
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, 
    test_size=0.2,        # 20% para test final
    random_state=42,      # Reproducibilidad
    stratify=y            # Mantiene proporciÃ³n de clases
)

# 2. Second split: 75% train, 25% val (del 80%)
# Resulta en 60/20/20 overall
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp,
    test_size=0.25,
    random_state=42,
    stratify=y_temp
)

print(f"Train: {len(X_train):,} samples (60%)")  # 6,300
print(f"Val:   {len(X_val):,} samples (20%)")    # 2,100
print(f"Test:  {len(X_test):,} samples (20%)")   # 2,100
```

**Â¿Por quÃ© 60/20/20?**
- âœ… Train (60%): Suficiente data para aprender patrones
- âœ… Val (20%): Validar sin tocar test set
- âœ… Test (20%): EvaluaciÃ³n final no sesgada

### Data Leakage Check

```python
def check_data_leakage(X_train, X_val, X_test):
    """
    Verifica que no haya samples duplicados entre splits
    """
    train_set = {tuple(row) for row in X_train}
    val_set = {tuple(row) for row in X_val}
    test_set = {tuple(row) for row in X_test}
    
    overlap_train_val = len(train_set & val_set)
    overlap_train_test = len(train_set & test_set)
    overlap_val_test = len(val_set & test_set)
    
    if overlap_train_val > 0:
        raise ValueError(f"Data leakage: {overlap_train_val} samples in train AND val")
    
    print("âœ… No data leakage detected")
```

**Â¿Por quÃ© importante?**
- âŒ Data leakage â†’ mÃ©tricas infladas, modelo no generaliza
- âœ… Sin leakage â†’ confianza en mÃ©tricas

### Training Process

```python
# 1. Initialize ensemble
ensemble = AnomalyEnsemble(
    contamination=0.05,  # 5% esperado de anomalÃ­as
    n_estimators=100,
    dbscan_eps=5.0,
    dbscan_min_samples=50,
    gmm_n_components=3,
    ensemble_weights=[0.5, 0.3, 0.2]
)

# 2. Train on training set
ensemble.train(X_train)  # 6,300 samples
# - Fits StandardScaler
# - Trains Isolation Forest
# - Trains DBSCAN + computes cluster centroids
# - Trains GMM

# Training time: ~3.5 segundos en CPU moderna

# 3. Validate on validation set
val_metrics = evaluate_model(ensemble, X_val, y_val)

# 4. Final evaluation on test set (ONLY ONCE!)
test_metrics = evaluate_model(ensemble, X_test, y_test)
```

**Â¿Por quÃ© NO tocar test set hasta el final?**
- âŒ Tuning con test set â†’ overfitting
- âœ… Test set virgen â†’ mÃ©tricas reales

---

## ğŸ“Š MÃ©tricas de Performance

### Confusion Matrix (Test Set)

```
              Predicted
           Normal  Anomaly
Actual
Normal      1993      7     â† FPR = 7/2000 = 0.35%
Anomaly        0    100     â† FNR = 0/100 = 0%
```

**InterpretaciÃ³n:**
- **TN (1993)**: Logs normales correctamente clasificados âœ…
- **FP (7)**: Falsos positivos - solo 7 de 2000 normales âœ…
- **FN (0)**: CERO amenazas perdidas ğŸ¯
- **TP (100)**: Todas las anomalÃ­as detectadas âœ…

### MÃ©tricas Principales

```python
# Test Set Results:
{
    "accuracy": 0.997,      # 99.7% - casi perfecto
    "precision": 0.935,     # 93.5% - cuando dice anomalÃ­a, acierta 93.5%
    "recall": 1.000,        # 100% - detecta TODAS las anomalÃ­as ğŸ¯
    "f1_score": 0.966,      # 96.6% - balance perfecto
    "roc_auc": 1.000,       # 100% - separaciÃ³n perfecta de clases
    "fpr": 0.0035,          # 0.35% - tasa muy baja de falsos positivos
    "fnr": 0.0              # 0% - NO pierde amenazas
}
```

**Â¿QuÃ© mÃ©trica es mÃ¡s importante?**

Para **seguridad**, la prioridad es:
1. **Recall (100%)**: NO podemos perder amenazas â†’ FN = 0 âœ…
2. **FPR (0.35%)**: Pocos falsos positivos para no saturar SOC âœ…
3. **F1-Score (96.6%)**: Balance general excelente âœ…

**ComparaciÃ³n con Baseline:**

```python
# Baseline (Dummy Classifier - Most Frequent)
{
    "accuracy": 0.952,  # Solo predice "normal" siempre
    "f1_score": 0.0     # NO detecta ninguna anomalÃ­a
}

# Nuestro Ensemble
{
    "accuracy": 0.997,  # +4.4 points
    "f1_score": 0.966   # +96.6 points (INFINITO en tÃ©rminos relativos)
}
```

**Conclusion:** El modelo es **MUCHO MEJOR** que baseline.

---

## ğŸš€ Deployment & Production

### Arquitectura en ProducciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Docker Compose Stack                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ postgres:5432  â†’ TimescaleDB (logs histÃ³ricos)       â”‚
â”‚  â€¢ redis:6379     â†’ Cache + rate tracking               â”‚
â”‚  â€¢ api:8000       â†’ FastAPI (ML inference)              â”‚
â”‚  â€¢ frontend:5173  â†’ React UI                            â”‚
â”‚  â€¢ prometheus:9090 â†’ Metrics collection                 â”‚
â”‚  â€¢ grafana:3000   â†’ Dashboards                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Loading

```python
# backend/ml/model_loader.py
class ModelLoader:
    def __init__(self):
        self.isolation_forest = None
        self.dbscan = None
        self.gmm = None
        self.scaler = None
        
    def load_model(self, model_path: Path) -> None:
        """
        Carga modelo entrenado desde disco
        """
        ensemble_data = joblib.load(model_path)
        
        self.isolation_forest = ensemble_data["isolation_forest"]
        self.dbscan = ensemble_data["dbscan"]
        self.gmm = ensemble_data["gmm"]
        self.scaler = ensemble_data["scaler"]
        
        # Metadata
        self.model_version = ensemble_data["model_version"]
        self.trained_at = ensemble_data["trained_at"]
        self.n_training_samples = ensemble_data["n_training_samples"]
```

### API Endpoints

```python
# GET /api/v1/health
{
    "status": "healthy",
    "checks": {
        "database": "healthy",
        "redis": "healthy",
        "ml_models": "loaded"
    }
}

# POST /api/v1/logs/analyze
Request:
{
    "log_line": "Jan 14 03:45:12 server sshd: Failed password...",
    "source": "auth"
}

Response:
{
    "is_anomaly": true,
    "risk_score": 0.834,
    "risk_level": "HIGH",
    "reasons": ["Unusual hour", "High login rate", ...],
    "recommended_action": "BLOCK_IP",
    "model_scores": {
        "isolation_forest": 0.85,
        "dbscan": 0.75,
        "gmm": 0.92
    },
    "processing_time_ms": 8.5
}
```

### Performance

```
# Latencia de inferencia:
- Parse: ~1ms
- Feature engineering: ~3ms (Redis + GeoIP)
- ML prediction: ~5ms (3 modelos)
- Total: ~8-10ms por log

# Throughput:
- 1 worker: ~100 logs/sec
- 4 workers: ~400 logs/sec
- Con batching: >1000 logs/sec
```

### Monitoring

```python
# Prometheus metrics
ml_predictions_total           # Counter de predicciones
ml_anomalies_detected_total    # Counter de anomalÃ­as
ml_prediction_duration_seconds # Histogram de latencia
ml_model_score_distribution    # Histogram de scores

# Grafana dashboards
- Real-time anomaly detection rate
- Model scores distribution
- False positive rate trends
- Processing latency
```

---

## ğŸ› Troubleshooting

### Problema: High False Positive Rate

**SÃ­ntoma:**
```
Muchos logs normales clasificados como anomalÃ­as
FPR > 5%
```

**Causas:**
1. **contamination** muy alto en Isolation Forest
2. **dbscan_eps** muy pequeÃ±o (todo es outlier)
3. **ensemble_weights** desbalanceados

**SoluciÃ³n:**
```python
# 1. Reducir contamination
ensemble = AnomalyEnsemble(
    contamination=0.03,  # Era 0.05, bajar a 3%
)

# 2. Aumentar dbscan_eps
ensemble = AnomalyEnsemble(
    dbscan_eps=7.0,  # Era 5.0, aumentar radio
)

# 3. Ajustar weights (dar mÃ¡s peso a IF)
ensemble = AnomalyEnsemble(
    ensemble_weights=[0.6, 0.2, 0.2]  # MÃ¡s conservador
)
```

### Problema: Missing Anomalies (Low Recall)

**SÃ­ntoma:**
```
Amenazas reales NO detectadas
FN > 0, Recall < 100%
```

**Causas:**
1. **threshold** muy alto
2. Modelo no entrenado con suficiente variedad
3. Features no capturan el patrÃ³n

**SoluciÃ³n:**
```python
# 1. Bajar threshold
ALERT_THRESHOLD_MEDIUM = 0.5  # Era 0.6

# 2. Re-entrenar con mÃ¡s data sintÃ©tica de ese ataque
anomalous_data["new_attack_pattern"] = ...

# 3. AÃ±adir nuevas features especÃ­ficas
is_sql_injection = check_sql_patterns(payload)
```

### Problema: Model Overfitting

**SÃ­ntoma:**
```
Validation accuracy >> Test accuracy
Model no generaliza a data nueva
```

**Causas:**
1. Data leakage entre splits
2. Overfitting en data sintÃ©tica
3. No suficiente diversidad en training data

**SoluciÃ³n:**
```python
# 1. Re-validar splits
check_data_leakage(X_train, X_val, X_test)

# 2. Aumentar varianza en data sintÃ©tica
anomalous_data["login_attempts"] = np.random.uniform(15, 50, n)  # MÃ¡s variedad

# 3. Cross-validation
from sklearn.model_selection import StratifiedKFold
kfold = StratifiedKFold(n_splits=5)
cv_scores = []
for train_idx, val_idx in kfold.split(X, y):
    ensemble.train(X[train_idx])
    score = evaluate(X[val_idx], y[val_idx])
    cv_scores.append(score)
print(f"CV F1: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}")
```

### Problema: Slow Inference

**SÃ­ntoma:**
```
processing_time_ms > 50ms
API timeouts
```

**Causas:**
1. Redis/Postgres queries lentas
2. GeoIP lookups sin cache
3. Sin workers paralelizaciÃ³n

**SoluciÃ³n:**
```python
# 1. Ãndices en Postgres
CREATE INDEX idx_logs_source_ip_timestamp ON logs(source_ip, log_timestamp);

# 2. Cache GeoIP lookups
@lru_cache(maxsize=10000)
def geoip_lookup(ip: str) -> Location:
    return geoip2.city(ip)

# 3. Aumentar workers
# docker-compose.yml
command: uvicorn backend.main:app --host 0.0.0.0 --port 8000 --workers 4
```

---

## ğŸ“š Referencias

### Papers

- Liu, Fei Tony, et al. "Isolation forest." 2008 eighth ieee international conference on data mining. IEEE, 2008.
- Ester, Martin, et al. "A density-based algorithm for discovering clusters in large spatial databases with noise." kdd. Vol. 96. No. 34. 1996.
- Reynolds, Douglas. "Gaussian mixture models." Encyclopedia of biometrics 741 (2009): 659-663.

### Libraries

- scikit-learn: https://scikit-learn.org/stable/
- FastAPI: https://fastapi.tiangolo.com/
- Structlog: https://www.structlog.org/

### Internal Docs

- `history/DATA_FLOW.md` - Flujo completo de data
- `history/INSTALL.md` - Setup con uv
- `README.md` - Overview del proyecto

---

## âœ… Checklist para ML Engineers

Al hacer cambios en el modelo:

```
â–¡ Modificar cÃ³digo en backend/ml/
â–¡ Re-entrenar con scripts/train_ensemble_with_metrics.py
â–¡ Verificar mÃ©tricas (F1 > 0.95, FNR = 0)
â–¡ Actualizar MODEL_PATH en .env
â–¡ Rebuild Docker: docker compose build
â–¡ Restart containers: docker compose up -d
â–¡ Verificar health: curl http://localhost:8000/api/v1/health
â–¡ Probar con send_test_logs.py
â–¡ Monitorear Grafana para FPR/FNR
â–¡ Documentar cambios en CHANGELOG.md
```

---

**ğŸ¯ Fin del ML Engineer Guide**

Para presentarlo a tus compaÃ±eros, te recomiendo:
1. Abrir el frontend: `http://localhost:5173`
2. Ir a la tab "ğŸ—ï¸ Architecture"
3. Usar este documento como referencia tÃ©cnica
4. Ejecutar `scripts/send_test_logs.py` para demo en vivo
